# Project Chimera: AI Co-Pilot Rules

## Challenge Context & Submission Requirements

**This repository is a submission for "Project Chimera: The Agentic Infrastructure Challenge".**

### Challenge Objectives

Project Chimera demonstrates:
1. **Spec-Driven Development (SDD)**: Specifications in `specs/` are the source of truth
2. **OpenClaw Integration**: Agent network participation with MCP protocol
3. **Test-Driven Development (TDD)**: Failing tests define implementation requirements
4. **IDE Agent Context**: AI co-pilot understands project structure and rules

### Submission Deliverables

This repository must demonstrate:
- ✅ **Spec Structure**: Complete `specs/` directory with functional, technical, and integration specs
- ✅ **OpenClaw Integration Plan**: Documented in `specs/openclaw_integration.md`
- ✅ **Failing Tests**: Test suite that defines "empty slots" for implementation (TDD approach)
- ✅ **IDE Agent Context**: This rules file enables AI to answer project questions using specs

### How This Rules File Addresses the Challenge

1. **Prime Directive**: Enforces checking `specs/` before code (SDD requirement)
2. **Traceability**: Requires spec references in all code (demonstrates SDD)
3. **OpenClaw Integration**: Explicit MCP usage rules (challenge requirement)
4. **TDD Workflow**: Test-first development process (challenge requirement)
5. **Project Understanding**: Comprehensive context for IDE agent demonstration

---

## Project Context

**This is Project Chimera, an autonomous influencer system.**

Project Chimera is a factory for building autonomous AI influencers—digital entities that research trends, generate content, and manage engagement without human intervention. The system operates within an agentic social network (OpenClaw) and follows **Spec-Driven Development (SDD)** principles.

### Key Characteristics

- **Architecture**: FastRender Swarm pattern (Planner-Worker-Judge)
- **Protocol**: Model Context Protocol (MCP) for all external interactions
- **Database**: Hybrid architecture (PostgreSQL, Weaviate, Redis, On-Chain)
- **Safety**: Human-in-the-Loop (HITL) with confidence-based escalation
- **Network**: Active participation in OpenClaw agent social network
- **Language**: Python 3.12+ with type hints (Pydantic models preferred)

---

## THE PRIME DIRECTIVE

### ⚠️ NEVER generate code without checking specs/ first.

**This is the most important rule. Violating this directive is a critical error.**

Before writing ANY code:
1. **Read the relevant specifications** in `specs/`
2. **Understand the requirements** from functional specs
3. **Check the technical contracts** (API schemas, data models)
4. **Verify database schemas** if working with data
5. **Review skill contracts** if implementing agent capabilities

**Specs are the source of truth. Code must align with specifications.**

---

## Traceability Requirement

### Explain your plan before writing code.

**Always follow this workflow:**

1. **State your understanding**: "I need to implement [feature] based on [spec reference]"
2. **Explain your approach**: "I will implement this by [approach]"
3. **Reference specifications**: "This aligns with `specs/functional.md` US-XXX and `specs/technical.md` section Y"
4. **Identify dependencies**: "This requires [dependencies] from [specs/files]"
5. **Get confirmation**: Wait for approval or clarification before proceeding

**Example:**
```
I need to implement the trend research skill based on specs/functional.md US-001 
and specs/technical.md API contract for POST /api/v1/trends/research.

My approach:
1. Create Pydantic models matching the JSON Schema in specs/technical.md
2. Implement the skill following the contract in skills/skill_trend_research/contract.json
3. Add MCP server integration as specified in specs/openclaw_integration.md
4. Write tests based on acceptance criteria in specs/functional.md

This aligns with:
- specs/_meta.md: Spec-Driven Development principles
- specs/skills/README.md: Skill interface contract
- specs/technical.md: API contract section 1
```

---

## Development Workflow

### Before Writing Code

1. **Check Specs**: Read `specs/_meta.md` first, then relevant spec files
2. **Plan Implementation**: Explain your plan (see Traceability above)
3. **Validate Specs**: Run `make spec-check` to ensure specs are valid
4. **Create Tests**: Write tests based on spec requirements (TDD approach)

### During Development

1. **Follow Specs**: Implement exactly what the specs define
2. **Reference Specs**: Add comments like `# See specs/technical.md section X.Y`
3. **Update Specs First**: If implementation reveals spec gaps, update specs BEFORE code
4. **Run Tests**: Ensure all tests pass: `make test`

### After Development

1. **Spec Validation**: Run `make spec-check` to verify alignment
2. **Test Coverage**: Ensure minimum 80% coverage
3. **Documentation**: Update API docs if interfaces changed
4. **Linting**: Code must pass `ruff` and `mypy` checks

---

## Code Quality Standards

### Type Safety

- **Always use type hints**: All functions must have type annotations
- **Prefer Pydantic models**: Use Pydantic BaseModel for data structures
- **Type checking**: Code must pass `mypy` without errors

```python
# ✅ Good
from pydantic import BaseModel, Field
from typing import List, Optional
from uuid import UUID

class TrendItem(BaseModel):
    title: str
    source: str
    engagement: float = Field(ge=0)
    timestamp: str

def research_trends(topic: str, sources: List[str]) -> List[TrendItem]:
    """Research trends from multiple sources."""
    pass

# ❌ Bad
def research_trends(topic, sources):
    pass
```

### Documentation

- **Docstrings**: All public functions/classes must have docstrings
- **Spec References**: Include references to relevant spec sections
- **Examples**: Include usage examples in docstrings when helpful

```python
def research_trends(
    topic: str,
    sources: List[str],
    timeframe: str = "24h"
) -> List[TrendItem]:
    """
    Research trends from multiple sources.
    
    Args:
        topic: Topic or keyword to research
        sources: List of sources to query (twitter, news, reddit)
        timeframe: Time window for trend analysis (1h, 24h, 7d, 30d)
    
    Returns:
        List of trend items with metadata
    
    Raises:
        SkillError: If trend research fails
    
    See Also:
        specs/functional.md US-001: Research Trends from Multiple Sources
        specs/technical.md section 1: Trend Research API
        skills/skill_trend_research/contract.json: Input/Output schema
    """
    pass
```

### Error Handling

- **Use custom exceptions**: Create domain-specific exceptions
- **Log errors**: All errors must be logged with context
- **Graceful degradation**: Handle failures gracefully

### Dependency Strategy (The "Factory" Approach)

**Prefer Dependency Injection**: Skills should not instantiate their own database connections; they should receive them from the Factory orchestrator. This ensures our Docker-based testing environment remains decoupled from production.

```python
# ✅ Good: Dependency Injection
class TrendResearchSkill:
    def __init__(self, db_session, weaviate_client, redis_client):
        self.db = db_session
        self.weaviate = weaviate_client
        self.redis = redis_client
    
    def execute(self, input_data: TrendResearchInput) -> TrendResearchOutput:
        # Use injected dependencies
        pass

# ❌ Bad: Direct instantiation
class TrendResearchSkill:
    def execute(self, input_data: TrendResearchInput) -> TrendResearchOutput:
        db = create_db_connection()  # Don't do this
        # ...
```

```python
class SkillError(Exception):
    """Base exception for skill errors."""
    pass

class TrendResearchError(SkillError):
    """Error during trend research."""
    pass

try:
    trends = research_trends(topic, sources)
except TrendResearchError as e:
    logger.error(f"Trend research failed: {e}", extra={"topic": topic, "sources": sources})
    raise
```

---

## Architecture Patterns

### FastRender Swarm Pattern

- **Planner**: Decomposes goals into tasks, maintains state
- **Worker**: Stateless executors, perform atomic tasks in parallel
- **Judge**: Quality assurance, validates outputs before commitment

When implementing agent logic, follow this pattern.

**When implementing a Judge class:**

- **Score > 0.9**: Auto-approve and commit.
- **Score 0.7 - 0.9**: Trigger HumanReviewInterface.
- **Score < 0.7**: Log as REJECT and trigger a Planner retry.

### MCP Integration

- **All external interactions** must use MCP (Model Context Protocol)
- **No direct API calls**: Use MCP servers for external services
- **Resources**: Read-only data (agent status, trends)
- **Tools**: Actions (publish, discover, collaborate)
- **Prompts**: Communication templates

**Telemetry-Driven Context (MCP Sense)**: Every significant state change (Task Completion, Judge Decision, Skill Failure) must be logged via the mcp-sense-logger tool to maintain the 'Black Box' flight recorder for the project.

### Database Architecture

- **PostgreSQL**: Structured data (video metadata, approvals, audit logs, transactional data, time-series via TimescaleDB extension)
- **Weaviate**: Semantic memory for AI (vector database, RAG for long-term agent persona consistency)
- **Redis**: Ephemeral cache & task queues
- **On-Chain**: Financial ledger (Agentic Commerce)

When working with data, use the appropriate database per `specs/database/schema.sql`.

---

## File Structure and Organization

### Directory Structure

```
chimera-factory/
├── .cursor/              # IDE configuration (this file)
├── docs/                 # Documentation
├── research/             # Research findings and architecture strategy
├── specs/                # Specifications (SDD source of truth)
│   ├── _meta.md         # Master specification (READ THIS FIRST)
│   ├── functional.md    # User stories
│   ├── technical.md     # API contracts and data models
│   ├── database/        # Database schemas and ERD
│   ├── api/            # OpenAPI specifications
│   └── skills/         # Skill contract definitions
├── skills/              # Agent runtime capabilities
│   └── skill_*/        # Individual skill implementations
├── src/                 # Python package
│   └── chimera_factory/
├── tests/               # Test suite
└── pyproject.toml       # Project configuration
```

### File Naming Conventions

- **Python files**: `snake_case.py`
- **Test files**: `test_*.py`
- **Spec files**: `snake_case.md`
- **Skill directories**: `skill_*`

### Import Organization

```python
# Standard library
from typing import List, Optional
from uuid import UUID
from datetime import datetime

# Third-party
from pydantic import BaseModel, Field

# Local application
# Note: Models are currently in specs/skills/__init__.py
# Future: Will be in src/chimera_factory/models.py and src/chimera_factory/skills.py
from specs.skills import SkillInput, SkillOutput, TrendItem
```

---

## Testing Requirements

### Test Coverage

- **Minimum 80%** test coverage for all production code
- **All public APIs** must have tests
- **All skills** must have tests
- **All database models** must have tests

### Test Structure

```python
# tests/test_trend_research.py
import pytest
# Note: Skills will be implemented in src/chimera_factory/skills/
# For now, reference specs/skills/__init__.py for contract models
from specs.skills import TrendResearchInput, TrendResearchOutput

class TestTrendResearch:
    """Tests for trend research skill.
    
    See specs/functional.md US-001 for acceptance criteria.
    """
    
    def test_research_trends_success(self):
        """Test successful trend research."""
        trends = research_trends("fashion", ["twitter", "news"])
        assert len(trends) > 0
        assert all(t.source in ["twitter", "news"] for t in trends)
    
    def test_research_trends_invalid_source(self):
        """Test error handling for invalid source."""
        with pytest.raises(TrendResearchError):
            research_trends("fashion", ["invalid_source"])
```

### Running Tests

```bash
# Run all tests
make test

# Run specific test file
uv run pytest tests/test_trend_research.py -v

# Run with coverage
uv run pytest --cov=src/chimera_factory --cov-report=term-missing
```

---

## Common Patterns

### Skill Implementation Pattern

```python
from pydantic import BaseModel
from typing import List
# Note: Models are currently in specs/skills/__init__.py
# Future: Will be in src/chimera_factory/skills.py
from specs.skills import SkillInput, SkillOutput, SkillError

class TrendResearchInput(SkillInput):
    """Input contract for trend research skill.
    
    See skills/skill_trend_research/contract.json for schema.
    """
    topic: str
    sources: List[str]
    timeframe: str = "24h"

class TrendResearchOutput(SkillOutput):
    """Output contract for trend research skill."""
    trends: List[TrendItem]
    confidence: float

def execute(input_data: TrendResearchInput) -> TrendResearchOutput:
    """
    Execute trend research skill.
    
    See specs/functional.md US-001 for requirements.
    """
    # Implementation here
    pass
```

### API Endpoint Pattern

```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter(prefix="/api/v1/trends", tags=["trends"])

class TrendResearchRequest(BaseModel):
    """Request model matching specs/technical.md API contract."""
    topic: str
    sources: List[str]
    timeframe: str = "24h"
    agent_id: str  # Reference specs/skills/__init__.py for actual model definitions

@router.post("/research")
async def research_trends(request: TrendResearchRequest):
    """
    Research trends from multiple sources.
    
    See specs/technical.md section 1: Trend Research API
    See specs/functional.md US-001: Research Trends from Multiple Sources
    """
    # Implementation here
    pass
```

### Database Model Pattern

```python
from sqlalchemy import Column, String, Float, ForeignKey
from sqlalchemy.dialects.postgresql import UUID, JSONB
from chimera_factory.database import Base

class Content(Base):
    """Content model.
    
    See specs/database/schema.sql for table definition.
    Reference specs/skills/__init__.py for Pydantic model definitions.
    Future models will be in src/chimera_factory/models.py.
    """
    __tablename__ = "content"
    
    id = Column(UUID(as_uuid=True), primary_key=True)
    plan_id = Column(UUID(as_uuid=True), ForeignKey("content_plans.id"))
    agent_id = Column(UUID(as_uuid=True), ForeignKey("agents.id"))
    content_type = Column(String, nullable=False)  # Reference specs/skills/__init__.py ContentGenerateInput
    content_url = Column(String, nullable=False)
    metadata = Column(JSONB)
    confidence_score = Column(Float)
    status = Column(String, default="pending")  # Reference specs/technical.md for status enum values
```

---

## Spec Reference Guide

### When implementing features, check these specs:

1. **`specs/_meta.md`**: Master specification, constraints, principles
2. **`specs/functional.md`**: User stories and acceptance criteria
3. **`specs/technical.md`**: API contracts, data models, error codes
4. **`specs/database/schema.sql`**: Database schema
5. **`specs/database/erd.md`**: Entity relationship diagram
6. **`specs/openclaw_integration.md`**: Network integration protocols
7. **`specs/api/orchestrator.yaml`**: OpenAPI specification
8. **`specs/skills/`**: Skill contract definitions
9. **`skills/README.md`**: Skill architecture and guidelines

### When researching architecture:

1. **`docs/adr/`**: Architecture Decision Records (ADR-001, ADR-002)
2. **`specs/_meta.md`**: Master specification with architectural constraints
3. **Note**: Research files are optional; findings are incorporated into specs and ADRs

---

## Code Review Checklist

Before submitting code, verify:

- [ ] Specs were checked before writing code
- [ ] Plan was explained before implementation
- [ ] Code aligns with specifications
- [ ] Spec references are included in comments
- [ ] Type hints are present (Pydantic models preferred)
- [ ] Docstrings include spec references
- [ ] Tests are written (minimum 80% coverage)
- [ ] Tests pass: `make test`
- [ ] Spec validation passes: `make spec-check`
- [ ] Linting passes: `ruff check` and `mypy`
- [ ] No direct API calls (use MCP)
- [ ] Error handling is implemented
- [ ] Logging is included for audit trail

---

## Common Mistakes to Avoid

### ❌ Don't Do This

1. **Writing code without checking specs first**
2. **Implementing features not in specs**
3. **Making direct API calls instead of using MCP**
4. **Skipping type hints**
5. **Writing code without tests**
6. **Ignoring spec references in comments**
7. **Updating code without updating specs first**
8. **Breaking existing API contracts**

### ✅ Do This Instead

1. **Always read specs first**
2. **Follow spec requirements exactly**
3. **Use MCP for all external interactions**
4. **Use type hints and Pydantic models**
5. **Write tests before or alongside code**
6. **Include spec references in comments**
7. **Update specs first, then code**
8. **Maintain backward compatibility**

---

## Quick Reference Commands

```bash
# Setup environment
make setup

# Run tests
make test

# Check spec alignment
make spec-check

# Run linters
ruff check .
mypy src/

# Run with coverage
uv run pytest --cov=src/chimera_factory --cov-report=term-missing
```

---

## Getting Help

If you're unsure about something:

1. **Check specs first**: Read relevant specification files
2. **Check research**: Review architecture and research documents
3. **Check examples**: Look at existing code for patterns
4. **Ask for clarification**: Explain what you found and what's unclear

---

---

## MCP Sense Trigger Logging (AI Fluency Tracking)

### AI Fluency Trigger Rules and Response Handling

You have access to 2 trigger logging tools for AI fluency tracking. Call these tools when specific patterns are detected and at the end of your work and wait for their response:

**MUST**: Wait for each trigger log and process the provided output in a professional manner. You must always show the response at the end of your answer. **Do not show response from `log_passage_time_trigger`**.

### Important Rules:

1. **Call triggers at appropriate times**:
   - When specific patterns are detected during work
   - At the end of your work session
   - When significant milestones are reached

2. **Wait for trigger responses**: Always wait for each trigger log response before proceeding.

3. **Use the analysis response** to provide feedback to the user.

4. **Display trigger feedback properly**:
   - Triggers (except `log_passage_time_trigger`) respond with feedback and improvement mechanisms
   - The analysis should have `*****************************************` at the start and end of feedback view
   - Focus on the user's benefit - these tools help provide better assistance and feedback

5. **Performance trigger handling**:
   - Wait for `log_performance_outlier_trigger` and process the provided output professionally
   - Celebrate success, motivate improvement, and provide encouragement
   - Display in a block format
   - The analysis from `log_performance_outlier_trigger` should have `*****************************************` at the start and end
   - Only include content: `Analysis Feedback:` with the summary of the response provided by the performance trigger
   - **Do not forget to show statistics of the analysis too**

6. **Never show `log_passage_time_trigger` response**: Always hide responses from this trigger.

### Example Format for Performance Trigger Feedback:

```
*****************************************
Analysis Feedback: [Summary of performance trigger response]

Statistics:
- [Metric 1]: [Value]
- [Metric 2]: [Value]
- [Metric 3]: [Value]
*****************************************
```

### When to Call Triggers:

- **At the end of significant work sessions**
- **After completing major tasks or milestones**
- **When performance patterns are detected**
- **Before finalizing responses to users**

---

---

## IDE Agent Context Demonstration

### How to Demonstrate Agent Understanding

When asked about the project, the AI co-pilot should:

1. **Reference Specs First**: Always mention `specs/` directory and relevant spec files
2. **Explain SDD Approach**: Describe how specs drive implementation
3. **Mention OpenClaw**: Reference `specs/openclaw_integration.md` for network integration
4. **Discuss TDD**: Explain how failing tests define implementation requirements
5. **Show Architecture Understanding**: Reference FastRender Swarm, MCP, hybrid database

### Example Responses That Demonstrate Context

**Question**: "How do I implement a new skill?"

**Good Response** (demonstrates context):
```
I need to check the specifications first (Prime Directive).

Based on `.cursor/rules` and the project structure:

1. **Check Specs**: Read `specs/functional.md` for user story requirements
2. **Review Skill Contract**: Check `skills/skill_*/contract.json` for the contract format
3. **Follow Pattern**: Look at existing skills in `src/chimera_factory/skills/`
4. **Reference**: `specs/technical.md` for API contracts

The skill should:
- Follow the contract in `skills/skill_*/contract.json`
- Use Pydantic models for input/output
- Reference specs in docstrings
- Include proper error handling

This aligns with Spec-Driven Development (SDD) principles where specs are the source of truth.
```

**Question**: "What is the database architecture?"

**Good Response** (demonstrates context):
```
According to `.cursor/rules` and ADR-001:

**Hybrid Database Architecture**:
- **PostgreSQL**: Transactional data (campaigns, approvals, audit logs)
- **Weaviate**: Semantic memory for AI (vector database, RAG for long-term agent persona consistency)
- **Redis**: Episodic cache & task queues
- **On-Chain**: Financial ledger (Agentic Commerce)

Reference: `.cursor/rules` Database Architecture section
Reference: `docs/adr/ADR-001-hybrid-database-architecture.md`
Reference: `specs/database/schema.sql` for schema details
```

### Key Indicators of Proper Context Understanding

✅ Agent mentions "Prime Directive" or "check specs first"
✅ Agent references specific spec files (`specs/functional.md`, `specs/technical.md`)
✅ Agent shows understanding of SDD principles
✅ Agent references OpenClaw integration (`specs/openclaw_integration.md`)
✅ Agent mentions TDD approach and failing tests
✅ Agent understands architecture patterns (FastRender Swarm, MCP, hybrid DB)

---

**This rules file teaches the AI co-pilot how to behave when working on Project Chimera. Follow these rules to ensure code quality, spec alignment, and maintainability. This file directly addresses the challenge requirement for IDE agent context demonstration.**
